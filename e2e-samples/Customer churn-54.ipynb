{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score a churn prediction model"],"metadata":{},"id":"98e321e3-7797-4a8d-9822-2f1084ba2e2f"},{"cell_type":"markdown","source":["## Introduction\n","\n","In this notebook, you'll see a Microsoft Fabric data science workflow with an end-to-end example. The scenario is to build a model to predict whether bank customers would churn or not. The churn rate, also known as the rate of attrition refers to the rate at which bank customers stop doing business with the bank.\n","\n","The main steps in this notebook are:\n","\n","1. Install custom libraries\n","2. Load the data\n","3. Understand and process the data through exploratory data analysis and demonstrate the use of Fabric Data Wrangler feature\n","4. Train machine learning models using `Scikit-Learn` and `LightGBM`, and track experiments using MLflow and Fabric Autologging feature\n","5. Evaluate and save the final machine learning model\n","6. Demonstrate the model performance via visualizations in Power BI\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"faa52bd2-a2ca-4cc4-90c2-e37b69df136d"},{"cell_type":"markdown","source":["## Prerequisites\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1eb8ee2b-ba0c-43b3-9d9d-8654067ec748"},{"cell_type":"markdown","source":["## Step 1: Install custom libraries\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d198b4dc-fd92-441f-9df2-5fe3266d09f3"},{"cell_type":"markdown","source":["When developing a machine learning model or doing ad-hoc data analysis, you may need to quickly install a custom library (e.g., `imblearn` in this notebook) for the Apache Spark session. To do this, you have two choices. \n","\n","1. You can use the in-line installation capabilities (e.g., `%pip`, `%conda`, etc.) to quickly get started with new libraries. Note that this installation option would install the custom libraries only in the current notebook and not in the workspace.\n","\n","```python\n","# Use pip to install libraries\n","%pip install <library name>\n","\n","# Use conda to install libraries\n","%conda install <library name>\n"," \n","```\n","2. Alternatively, you can follow the instructions [here](https://aka.ms/fabric/create-environment) to learn how to create an environment which allows you to install libraries from public sources or upload custom libraries built by you or your organization."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"24e40fc3-322e-4f19-9218-1636d3f9e082"},{"cell_type":"markdown","source":["For this notebook, you'll install the `imblearn` using `%pip install`. Note that the PySpark kernel will be restarted after `%pip install`, thus you'll need to install the library before you run any other cells."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"591a8c19-19ab-49ce-b1b8-7c3cbfe9a00a"},{"cell_type":"code","source":["# Use pip to install imblearn for SMOTE\n","%pip install imblearn"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8b5da681-e3ed-4ddc-8ff1-5cea41a3f98a"},{"cell_type":"markdown","source":["## Step 2: Load the data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7eccc566-de46-4769-91d1-0c68256c90a4"},{"cell_type":"markdown","source":["### Dataset\n","\n","The dataset contains churn status of 10,000 customers along with 14 attributes that include credit score, geographical location (Germany, France, Spain), gender (male, female), age, tenure (years of being bank's customer), account balance, estimated salary, number of products that a customer has purchased through the bank, credit card status (whether a customer has a credit card or not), and active member status (whether an active bank's customer or not).\n","\n","The dataset also includes columns such as row number, customer ID, and customer surname that should have no impact on customer's decision to leave the bank. The event that defines the customer's churn is the closing of the customer's bank account, therefore, the column `exit` in the dataset refers to customer's abandonment. Since you don't have much context about these attributes, you'll proceed without having background information about the dataset. Your aim is to understand how these attributes contribute to the `exit` status.\n","\n","Out of the 10,000 customers, only 2037 customers (around 20%) have left the bank. Therefore, given the class imbalance ratio, it is recommended to generate synthetic data.\n","\n","- churn.csv\n","\n","|\"CustomerID\"|\"Surname\"|\"CreditScore\"|\"Geography\"|\"Gender\"|\"Age\"|\"Tenure\"|\"Balance\"|\"NumOfProducts\"|\"HasCrCard\"|\"IsActiveMember\"|\"EstimatedSalary\"|\"Exited\"|\n","|---|---|---|---|---|---|---|---|---|---|---|---|---|\n","|15634602|Hargrave|619|France|Female|42|2|0.00|1|1|1|101348.88|1|\n","|15647311|Hill|608|Spain|Female|41|1|83807.86|1|0|1|112542.58|0|\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90eec3b3-1a5d-4a3a-bfbd-78b0b035f7a9"},{"cell_type":"markdown","source":["### Introduction to SMOTE\n","\n","The problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. Synthetic Minority Oversampling Technique (SMOTE) is the most widely used approach to synthesize new samples for the minority class. Learn more about SMOTE [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#) and [here](https://imbalanced-learn.org/stable/over_sampling.html#smote-adasyn).\n","\n","You will be able to access SMOTE using the `imblearn` library that you installed in Step 1."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8639514c-bbb8-49aa-9091-f7df34a26d91"},{"cell_type":"markdown","source":["### Download dataset and upload to lakehouse\n","\n","> [!TIP]\n","> By defining the following parameters, you can use this notebook with different datasets easily.\n"],"metadata":{},"id":"26e0b468"},{"cell_type":"code","source":["IS_CUSTOM_DATA = False  # if TRUE, dataset has to be uploaded manually\n","\n","IS_SAMPLE = False  # if TRUE, use only SAMPLE_ROWS of data for training, otherwise use all data\n","SAMPLE_ROWS = 5000  # if IS_SAMPLE is True, use only this number of rows for training\n","\n","DATA_ROOT = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/churn\"  # folder with data files\n","DATA_FILE = \"churn.csv\"  # data file name"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bcd5f6a9-2008-4a06-83cd-a0874797a801"},{"cell_type":"markdown","source":["\n","\n","This code downloads a publicly available version of the dataset and then stores it in a Fabric lakehouse.\n","\n","> [!IMPORTANT]\n","> **Make sure you [add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. Failure to do so will result in an error.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"20376a28-ff40-47a7-86a0-7fb272c88028"},{"cell_type":"code","source":["import os, requests\n","if not IS_CUSTOM_DATA:\n","# Using synapse blob, this can be done in one line\n","\n","# Download demo data files into lakehouse if not exist\n","    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn\"\n","    file_list = [\"churn.csv\"]\n","    download_path = \"/lakehouse/default/Files/churn/raw\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\n","            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","        )\n","    os.makedirs(download_path, exist_ok=True)\n","    for fname in file_list:\n","        if not os.path.exists(f\"{download_path}/{fname}\"):\n","            r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n","            with open(f\"{download_path}/{fname}\", \"wb\") as f:\n","                f.write(r.content)\n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a89670b9-66cb-4679-84a1-bf36923d26a6"},{"cell_type":"markdown","source":["Start recording the time it takes to run this notebook."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d1e6b5c-5591-478c-aafb-6ed093b70e0b"},{"cell_type":"code","source":["# Record the notebook running time\n","import time\n","\n","ts = time.time()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2994fa0c-4a95-48e1-b463-f2eb72c1846b"},{"cell_type":"markdown","source":["### Read raw data from the lakehouse\n","\n","Reads raw data from the **Files** section of the lakehouse, adds additional columns for different date parts and the same information will be used to create partitioned delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1f22364a-ccb5-4686-9ac0-1120b859fbd5"},{"cell_type":"code","source":["df = (\n","    spark.read.option(\"header\", True)\n","    .option(\"inferSchema\", True)\n","    .csv(\"Files/churn/raw/churn.csv\")\n","    .cache()\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2cfd34fd-047c-44d0-884a-4941482bd5fb"},{"cell_type":"markdown","source":["### Create a pandas dataframe from the dataset\n","\n","This code converts the spark DataFrame to pandas DataFrame for easier processing and visualization."],"metadata":{},"id":"51a5c225-f482-4703-9b91-5cb6ba1970ae"},{"cell_type":"code","source":["df = df.toPandas()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d550d94b-552e-41fd-b23f-3caa064f842a"},{"cell_type":"markdown","source":["## Step 3: Exploratory Data Analysis"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"44379940-befb-4b21-a377-41ba78366ca3"},{"cell_type":"markdown","source":["### Display raw data\n","\n","Explore the raw data with `display`, do some basic statistics and show chart views. You first need to import required libraries for data visualization such as `seaborn` which is a Python data visualization library to provide a high-level interface for building visuals on dataframes and arrays. Learn more about [`seaborn`](https://seaborn.pydata.org/). "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cf0eb500-e05e-4e62-b297-0189a31d1037"},{"cell_type":"code","source":["import seaborn as sns\n","sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mticker\n","from matplotlib import rc, rcParams\n","import numpy as np\n","import pandas as pd\n","import itertools"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"73290de5-ed00-415a-b546-5f4a7586cff7"},{"cell_type":"code","source":["display(df, summary=True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7e4e1172-c117-4a3e-8d40-e3c983f750c9"},{"cell_type":"markdown","source":["### Use Data Wrangler to perform initial data cleansing\n","\n","Launch Data Wrangler directly from the notebook to explore and transform any pandas dataframe. Under the notebook ribbon `Data` tab, you can use the Data Wrangler dropdown prompt to browse the activated pandas DataFrames available for editing and select the one you wish to open in Data Wrangler. \n","\n",">[!NOTE]\n",">Data Wrangler can not be opened while the notebook kernel is busy. The cell execution must complete prior to launching Data Wrangler. [Learn more about Data Wrangler](https://aka.ms/fabric/datawrangler).\n","\n","<br>\n","\n","<img src=\"https://sdkstorerta.blob.core.windows.net/churnblob/select_datawrangler.png\"  width=\"40%\" height=\"10%\" title=\"Screenshot shows where to access the Data Wrangler.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d5f34273-fae3-4de1-ac85-1807146a121c"},{"cell_type":"markdown","source":["Once the Data Wrangler is launched, a descriptive overview of the displayed data panel is generated as shown in the following images. It includes information about the DataFrame's dimension, missing values, etc. You can then use Data Wrangler to generate the script for dropping the rows with missing values, the duplicate rows and the columns with specific names, then copy the script into a cell.  The next cell shows that copied script.\n","\n","\n","<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/menu_datawrangler.png\"  width=\"45%\" height=\"10%\" title=\"Screenshot shows Data Wrangler menu.\"> \n","<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/missing_data_datawrangler.png\"  width=\"45%\" height=\"10%\" title=\"Screenshot shows Data Wrangler missing data display.\">\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eacf1362-b8fc-403d-a0f4-42b3c369d303"},{"cell_type":"code","source":["def clean_data(df):\n","    # Drop rows with missing data across all columns\n","    df.dropna(inplace=True)\n","    # Drop duplicate rows in columns: 'RowNumber', 'CustomerId'\n","    df.drop_duplicates(subset=['RowNumber', 'CustomerId'], inplace=True)\n","    # Drop columns: 'RowNumber', 'CustomerId', 'Surname'\n","    df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)\n","    return df\n","\n","df_clean = clean_data(df.copy())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18014588-2616-4753-8c17-b115aab07310"},{"cell_type":"code","source":["# if IS_SAMPLE is True, use only SAMPLE_ROWS of rows for training\n","if IS_SAMPLE:\n","    df_clean = df_clean.limit(SAMPLE_ROWS)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4e92aa6e-bf04-49e6-b4d4-bb3373f09ae3"},{"cell_type":"markdown","source":["##### Determine attributes\n","\n","Use this code to determine categorical, numerical, and target attributes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2922660b-dd45-4c2d-8a75-7431fefffb0c"},{"cell_type":"code","source":["# Determine the dependent (target) attribute\n","dependent_variable_name = \"Exited\"\n","print(dependent_variable_name)\n","# Determine the categorical attributes\n","categorical_variables = [col for col in df_clean.columns if col in \"O\"\n","                        or df_clean[col].nunique() <=5\n","                        and col not in \"Exited\"]\n","print(categorical_variables)\n","# Determine the numerical attributes\n","numeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != \"object\"\n","                        and df_clean[col].nunique() >5]\n","print(numeric_variables)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3adb4922-607e-4f4e-bdb2-bda535acb94c"},{"cell_type":"markdown","source":["\n","##### The five-number summary \n","\n","Show the five-number summary (the minimum score, first quartile, median, third quartile, the maximum score) for the numerical attributes, using box plots."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0640a89c-6a64-4978-89f1-5a45cdd1303e"},{"cell_type":"code","source":["df_num_cols = df_clean[numeric_variables]\n","sns.set(font_scale = 0.7) \n","fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw =  dict(hspace=0.3), figsize = (17,8))\n","fig.tight_layout()\n","for ax,col in zip(axes.flatten(), df_num_cols.columns):\n","    sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n","# fig.suptitle('visualize and compare the distribution and central tendency of numerical attributes', color = 'k', fontsize = 12)\n","fig.delaxes(axes[1,2])\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f45982d4-de27-43b5-b826-e93730d7e66c"},{"cell_type":"markdown","source":["##### Distribution of exited and non-exited customers \n","\n","Show the distribution of exited versus non-exited customers across the categorical attributes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a320fd15-64eb-493e-9b60-500ad4535435"},{"cell_type":"code","source":["attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure']\n","fig, axarr = plt.subplots(2, 3, figsize=(15, 4))\n","for ind, item in enumerate (attr_list):\n","    sns.countplot(x = item, hue = 'Exited', data = df_clean, ax = axarr[ind%2][ind//2])\n","fig.subplots_adjust(hspace=0.7)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c4811cab-6cdc-4014-a0ab-8f90232e6603"},{"cell_type":"markdown","source":["##### Distribution of numerical attributes\n","\n","Show the the frequency distribution of numerical attributes using histogram.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5075fddc-1cce-4c64-8d17-499a309440d7"},{"cell_type":"code","source":["columns = df_num_cols.columns[: len(df_num_cols.columns)]\n","fig = plt.figure()\n","fig.set_size_inches(18, 8)\n","length = len(columns)\n","for i,j in itertools.zip_longest(columns, range(length)):\n","    plt.subplot((length // 2), 3, j+1)\n","    plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n","    df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n","    plt.title(i)\n","# fig = fig.suptitle('distribution of numerical attributes', color = 'r' ,fontsize = 14)\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8626b081-a301-4cfa-9db3-38ab8524879d"},{"cell_type":"markdown","source":["### Perform feature engineering \n","\n","The following feature engineering generates new attributes based on current attributes."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"945e1166-0e88-4baa-ac2c-b56b7eeeb7c4"},{"cell_type":"code","source":["df_clean[\"NewTenure\"] = df_clean[\"Tenure\"]/df_clean[\"Age\"]\n","df_clean[\"NewCreditsScore\"] = pd.qcut(df_clean['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\n","df_clean[\"NewAgeScore\"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n","df_clean[\"NewBalanceScore\"] = pd.qcut(df_clean['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n","df_clean[\"NewEstSalaryScore\"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e8a3e0c5-db7b-44bf-b11b-8d323934e151"},{"cell_type":"markdown","source":["### Use Data Wrangler to perform one-hot encoding\n","\n","Following the same instructions discussed earlier to launch Data Wrangler, use the Data Wrangler to perform one-hot encoding. The next cell shows the copied  generated script for one-hot encoding.\n","\n","<br>\n","\n","<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/1hotencoding_data_wrangler.png\"  width=\"45%\" height=\"20%\" title=\"Screenshot shows one-hot encoding in the Data Wrangler\"> \n","<img style=\"float: left;\" src=\"https://sdkstorerta.blob.core.windows.net/churnblob/1hotencoding_selectcolumns_data_wrangler.png\"  width=\"45%\" height=\"20%\" title=\"Screenshot shows selection of columns in the Data Wrangler.\">\n","\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"76be9b0c-3f67-461f-8bd7-cd0450b2bc1e"},{"cell_type":"code","source":["df_clean = pd.get_dummies(df_clean, columns=['Geography', 'Gender'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2b924a18-faf5-4b0b-8b57-3480f2f93083"},{"cell_type":"markdown","source":["### Create a delta table to generate the Power BI report"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5c4f77d9-1167-460e-a3ba-3268acafd285"},{"cell_type":"code","source":["table_name = \"df_clean\"\n","# Create PySpark DataFrame from Pandas\n","sparkDF=spark.createDataFrame(df_clean) \n","sparkDF.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9061366a-dd4f-43e1-9b81-081f43d20378"},{"cell_type":"markdown","source":["### Summary of observations from the exploratory data analysis\n","\n","- Most of the customers are from France comparing to Spain and Germany, while Spain has the lower churn rate comparing to France and Germany.\n","- Most of the customers have credit cards.\n","- There are customers whose age and credit score are above 60 and below 400, respectively, but they can't be considered as outliers.\n","- Very few customers have more than two of the bank's products.\n","- Customers who aren't active have a higher churn rate.\n","- Gender and tenure years don't seem to have an impact on customer's decision to close the bank account."],"metadata":{},"id":"1efc8976-b1e1-4c85-b4ee-8dd29040ca02"},{"cell_type":"markdown","source":["## Step 4: Model training and tracking\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"80113f25-b8d2-4d79-ac84-6c396dedb539"},{"cell_type":"markdown","source":["With your data in place, you can now define the model. You'll apply Random Forrest and LightGBM models in this notebook. \n","\n","Use `scikit-learn` and `lightgbm` to implement the models within a few lines of code. Also use MLfLow and Fabric Autologging to track the experiments.\n","\n","Here you'll load the delta table from the lakehouse. You may use other delta tables considering the lakehouse as the source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a5d1fffa-b0fb-4ac8-9d7f-41339ca50faf"},{"cell_type":"code","source":["SEED = 12345\n","df_clean = spark.read.format(\"delta\").load(\"Tables/df_clean\").toPandas()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"025e525b-b00c-4537-9edb-cc445782d736"},{"cell_type":"markdown","source":["### Generate experiment for tracking and logging the models using MLflow\n","\n","This section demonstrates how to generate an experiment, specify model and training parameters as well as scoring metrics, train the models, log them, and save the trained models for later use."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eef9eead-c909-4cb2-848f-12d2ad6a3df3"},{"cell_type":"code","source":["import mlflow\n","\n","# Set up experiment name\n","EXPERIMENT_NAME = \"sample-bank-churn-experiment\"  # MLflow experiment name"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"33810bac-86b5-46a6-9c37-cd28ca679a43"},{"cell_type":"markdown","source":["Extending the MLflow autologging capabilities, autologging works by automatically capturing the values of input parameters and output metrics of a machine learning model as it is being trained. This information is then logged to your workspace, where it can be accessed and visualized using the MLflow APIs or the corresponding experiment in your workspace. To learn more about  autologging, see  [Autologging in Microsoft Fabric](https://aka.ms/fabric-autologging)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6b415622-97f7-480f-bc72-9f63053742fc"},{"cell_type":"markdown","source":["### Set experiment and autologging specifications"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"65b8693b-2970-4daf-ad31-067a25b80b60"},{"cell_type":"code","source":["mlflow.set_experiment(EXPERIMENT_NAME) # Use date stamp to append to experiment\n","mlflow.autolog(exclusive=False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b08b40bc-e4c1-4faf-a388-2ed301580d16"},{"cell_type":"markdown","source":["### Import scikit-learn and LightGBM"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"634163fa-5062-469e-a5e9-ed49ef7e4607"},{"cell_type":"code","source":["# Import the required libraries for model training\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score, classification_report"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ffbdd7fd-a273-458c-9cea-dbcd583911dc"},{"cell_type":"markdown","source":["### Prepare training and test datasets"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c72af93c-971d-4b13-9079-0025a5d78b32"},{"cell_type":"code","source":["y = df_clean[\"Exited\"]\n","X = df_clean.drop(\"Exited\",axis=1)\n","# Train-Test Separation\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3c539eed-0f88-4f0a-b7d2-901c2f9930e3"},{"cell_type":"markdown","source":["### Apply SMOTE to the training data to synthesize new samples for the minority class\n","\n","SMOTE should only be applied to the training dataset. You must leave the test dataset in its original imbalanced distribution in order to get a valid approximation of how the model will perform on the original data, which is representing the situation in production."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c907a17c-ed92-4334-accc-2deec809334c"},{"cell_type":"code","source":["from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","\n","sm = SMOTE(random_state=SEED)\n","X_res, y_res = sm.fit_resample(X_train, y_train)\n","new_train = pd.concat([X_res, y_res], axis=1)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bdbb6dec-3e17-498a-8e8d-380bc9a59f5a"},{"cell_type":"markdown","source":["### Model training\n","\n","Train the model using Random Forest with maximum depth of four, with four features."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"039ad12c-e176-4162-80da-f1cd15becaf2"},{"cell_type":"code","source":["mlflow.sklearn.autolog(registered_model_name='rfc1_sm')  # Register the trained model with autologging\n","rfc1_sm = RandomForestClassifier(max_depth=4, max_features=4, min_samples_split=3, random_state=1) # Pass hyperparameters\n","with mlflow.start_run(run_name=\"rfc1_sm\") as run:\n","    rfc1_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n","    print(\"run_id: {}; status: {}\".format(rfc1_sm_run_id, run.info.status))\n","    # rfc1.fit(X_train,y_train) # imbalanaced training data\n","    rfc1_sm.fit(X_res, y_res.ravel()) # balanced training data\n","    rfc1_sm.score(X_test, y_test)\n","    y_pred = rfc1_sm.predict(X_test)\n","    cr_rfc1_sm = classification_report(y_test, y_pred)\n","    cm_rfc1_sm = confusion_matrix(y_test, y_pred)\n","    roc_auc_rfc1_sm = roc_auc_score(y_res, rfc1_sm.predict_proba(X_res)[:, 1])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8cfb7902-c40f-45a3-a7a1-6ab036bc19bc"},{"cell_type":"markdown","source":["Train the model using Random Forest with maximum depth of eight, with six features."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"562edcf7-8926-4757-ad5a-871a1f813772"},{"cell_type":"code","source":["mlflow.sklearn.autolog(registered_model_name='rfc2_sm')  # Register the trained model with autologging\n","rfc2_sm = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=3, random_state=1) # Pass hyperparameters\n","with mlflow.start_run(run_name=\"rfc2_sm\") as run:\n","    rfc2_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n","    print(\"run_id: {}; status: {}\".format(rfc2_sm_run_id, run.info.status))\n","    # rfc2.fit(X_train,y_train) # imbalanced training data\n","    rfc2_sm.fit(X_res, y_res.ravel()) # balanced training data\n","    rfc2_sm.score(X_test, y_test)\n","    y_pred = rfc2_sm.predict(X_test)\n","    cr_rfc2_sm = classification_report(y_test, y_pred)\n","    cm_rfc2_sm = confusion_matrix(y_test, y_pred)\n","    roc_auc_rfc2_sm = roc_auc_score(y_res, rfc2_sm.predict_proba(X_res)[:, 1])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18ab69b4-7098-4bfb-a393-fcd62a2bc0b4"},{"cell_type":"markdown","source":["Train the model using LightGBM."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"46086429-c184-46b8-872b-58632eae583e"},{"cell_type":"code","source":["# lgbm_model\n","mlflow.lightgbm.autolog(registered_model_name='lgbm_sm')  # Register the trained model with autologging\n","lgbm_sm_model = LGBMClassifier(learning_rate = 0.07, \n","                        max_delta_step = 2, \n","                        n_estimators = 100,\n","                        max_depth = 10, \n","                        eval_metric = \"logloss\", \n","                        objective='binary', \n","                        random_state=42)\n","\n","with mlflow.start_run(run_name=\"lgbm_sm\") as run:\n","    lgbm1_sm_run_id = run.info.run_id # Capture run_id for model prediction later\n","    # lgbm_sm_model.fit(X_train,y_train) # imbalanced training data\n","    lgbm_sm_model.fit(X_res, y_res.ravel()) # balanced training data\n","    y_pred = lgbm_sm_model.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    cr_lgbm_sm = classification_report(y_test, y_pred)\n","    cm_lgbm_sm = confusion_matrix(y_test, y_pred)\n","    roc_auc_lgbm_sm = roc_auc_score(y_res, lgbm_sm_model.predict_proba(X_res)[:, 1])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9f7d92a2-17ea-43b0-b17c-b3fac5b76af6"},{"cell_type":"markdown","source":["### Experiments artifact for tracking model performance\n","\n","The experiment runs are automatically saved in the experiment artifact that can be found from the workspace. They're named based on the name used for setting the experiment. All of the trained models, their runs, performance metrics and model parameters are logged as can be seen from the experiment page shown in the image below.   \n","\n","To view your experiments:\n","1. On the left panel, select your workspace.\n","1. Find and select the experiment name, in this case _sample-bank-churn-experiment_.\n","\n","<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn/experiment_runs.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"449f8d4d-04ac-43a8-a6e4-1c10ace0c376"},{"cell_type":"markdown","source":["## Step 5: Evaluate and save the final machine learning model\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90431a0d-205d-4352-9205-c98ef3b905c4"},{"cell_type":"markdown","source":["Open the saved experiment from the workspace to select and save the best model."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c61dd49a-dbc4-41b2-9065-58acd9072a5a"},{"cell_type":"code","source":["# Define run_uri to fetch the model\n","# mlflow client: mlflow.model.url, list model\n","load_model_rfc1_sm = mlflow.sklearn.load_model(f\"runs:/{rfc1_sm_run_id}/model\")\n","load_model_rfc2_sm = mlflow.sklearn.load_model(f\"runs:/{rfc2_sm_run_id}/model\")\n","load_model_lgbm1_sm = mlflow.lightgbm.load_model(f\"runs:/{lgbm1_sm_run_id}/model\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6f5dd987-2bc0-4ece-839d-d1a1fe66b6f1"},{"cell_type":"markdown","source":["### Assess the performances of the saved models on test dataset"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5803d3b4-3c9f-4d62-bc2a-79f841ec45cc"},{"cell_type":"code","source":["ypred_rfc1_sm = load_model_rfc1_sm.predict(X_test) # Random Forest with max depth of 4 and 4 features\n","ypred_rfc2_sm = load_model_rfc2_sm.predict(X_test) # Random Forest with max depth of 8 and 6 features\n","ypred_lgbm1_sm = load_model_lgbm1_sm.predict(X_test) # LightGBM"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0181064b-34b6-4f26-879d-9d145edfac06"},{"cell_type":"markdown","source":["### Show True/False Positives/Negatives using the Confusion Matrix\n","\n","Develop a script to plot the confusion matrix in order to evaluate the accuracy of the classification. You can also plot a confusion matrix using SynapseML tools, which is shown in the [Fraud Detection sample](https://aka.ms/samples/frauddectection)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"97eb022f-cf7a-4b1e-8c5c-d0c5dfe4a9f1"},{"cell_type":"code","source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    print(cm)\n","    plt.figure(figsize=(4,4))\n","    plt.rcParams.update({'font.size': 10})\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n","    plt.yticks(tick_marks, classes, color=\"blue\")\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"red\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"53ae34cf-eed2-457f-8088-00f73acedd3b"},{"cell_type":"markdown","source":["Create a confusion matrix for Random Forest Classifier with maximum depth of four, with four features.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b394b5b7-f4ca-4838-b77f-8ef8bdfd409f"},{"cell_type":"code","source":["cfm = confusion_matrix(y_test, y_pred=ypred_rfc1_sm)\n","plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n","                      title='Random Forest with max depth of 4')\n","tn, fp, fn, tp = cfm.ravel()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"89c19e11-fcab-4cd4-96d1-1f97a1ac7215"},{"cell_type":"markdown","source":["Create a confusion matrix for Random Forest Classifier with maximum depth of eight, with six features."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"50a20d7f-d9cb-42ca-a537-1b50894bedfb"},{"cell_type":"code","source":["cfm = confusion_matrix(y_test, y_pred=ypred_rfc2_sm)\n","plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n","                      title='Random Forest with max depth of 8')\n","tn, fp, fn, tp = cfm.ravel()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c5fefc40-de64-4480-8f21-0efd11538f93"},{"cell_type":"markdown","source":["Create the confusion matrix for LightGBM."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ef73db99-3670-44cb-8e15-f3885ca9a02b"},{"cell_type":"code","source":["cfm = confusion_matrix(y_test, y_pred=ypred_lgbm1_sm)\n","plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n","                      title='LightGBM')\n","tn, fp, fn, tp = cfm.ravel()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"05052942-cb51-461e-861c-8312bc6182d8"},{"cell_type":"markdown","source":["### Save results for Power BI\n","\n","Move model prediction results to Power BI Visualization by saving delta frame to lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a23cb929-aa96-49f4-8d16-85b93fab6a51"},{"cell_type":"code","source":["df_pred = X_test.copy()\n","df_pred['y_test'] = y_test\n","df_pred['ypred_rfc1_sm'] = ypred_rfc1_sm\n","df_pred['ypred_rfc2_sm'] =ypred_rfc2_sm\n","df_pred['ypred_lgbm1_sm'] = ypred_lgbm1_sm\n","table_name = \"df_pred_results\"\n","sparkDF=spark.createDataFrame(df_pred)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(f\"Tables/{table_name}\")\n","print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2b27a0b3-5647-463f-b26e-4edbf44bcde8"},{"cell_type":"markdown","source":["## Step 6: Business Intelligence via Visualizations in Power BI"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0d503f0e-6758-47ea-b836-8bc955663b20"},{"cell_type":"markdown","source":["Use these steps to access your saved table in Power BI.\n","\n","1. On the left, select **OneLake data hub**.\n","1. Select the lakehouse that you added to this notebook.\n","1. On the top right, select **Open** under the section titled **Open this Lakehouse**.\n","1. Select New Power BI dataset on the top ribbon and select `df_pred_results`, then select **Continue** to create a new Power BI dataset linked to the predictions.\n","1. On the tools at the top of the dataset page, select **New report** to open the Power BI report authoring page.\n","\n","Some example visualizations are shown here. The data panel shows the delta tables and columns from the table to select. Upon selecting appropriate x and y axes, you can pick the filters and functions, for example, sum or average of the table column.\n","\n","> [!NOTE]\n","> This shows an illustrated example of how you would analyze the saved prediction results in Power BI. However, for a real customer churn use-case, the platform user may have to do more thorough ideation of what visualizations to create, based on subject matter expertise, and what their firm and business analytics team has standardized as metrics.\n","\n","<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn/PBIviz3.png\"  width=\"100%\" height=\"100%\" title=\"Screenshot shows a Power BI dashboard example.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5a9e488f-cd7f-4076-a90b-2e8687aa3035"},{"cell_type":"markdown","source":["The Power BI report shows that customers who use more than two of the bank products have a higher churn rate although few customers had more than two products. The bank should collect more data, but also investigate other features correlated with more products (see the plot in the bottom left panel).\n","Bank customers in Germany have a higher churn rate than in France and Spain (see the plot in the bottom right panel), which suggests that an investigation into what has encouraged customers to leave could be beneficial.\n","There are more middle aged customers (between 25-45) and customers between 45-60 tend to exit more.\n","Finally, customers with lower credit scores would most likely leave the bank for other financial institutes. The bank should look into ways that encourage customers with lower credit scores and account balances to stay with the bank."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"61e3d32f-7fe4-4c31-ba4f-593ec631c542"},{"cell_type":"code","source":["# Determine the entire runtime\n","print(f\"Full run cost {int(time.time() - ts)} seconds.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"01f8f9d6-017e-4baa-b593-45bbdcf1d47e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}}},"nbformat":4,"nbformat_minor":5}