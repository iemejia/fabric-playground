{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score an uplift model"],"metadata":{},"id":"af6c52ee-d000-4328-83c3-897c06ca276b"},{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook demonstrates the data science work flow with an end-to-end example. The notebook creates, trains, and evaluates an uplift model that refers to a collection of machine learning techniques. These techniques are used to estimate the incremental effects of a treatment on the behavior of an individual or a sub-group. \n","\n","This notebook covers these topics:\n","\n","1. Load the data\n","2. Understand and process the data through exploratory data analysis\n","3. Train a machine learning model with an uplift model\n","4. Save the final machine learning model\n","5. Load the machine learning model for scoring and making predictions\n","\n","## Prerequisites\n","\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will download data from a public blob, then store the data in the lakehouse resource."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9cfd8e82-be33-4c90-aac7-47d397862ebc"},{"cell_type":"markdown","source":["### What is uplift modeling?\n","\n","Uplift modeling relies on a set of causal inference techniques that utilize machine learning models to identify the most suitable treatment, from a group of possible treatments, that leads to the most desirable outcome. Study of both a treatment group - individuals who undergo a treatment - and a control group - individuals who do not undergo that treatment - determines outcome desirability. This approach classifies individuals into four main subgroups within a population:\n","\n","- **Persuadables**: This group of individuals will respond positively only when given the treatment.\n","- **Sleeping-dogs**: This group of individuals will show a strong negative response to the treatment.\n","- **Lost Causes**: This group of individuals will never achieve the desired outcome, even with the treatment.\n","- **Sure Things**: This group of individuals will achieve the outcome regardless of whether they receive the treatment or not.\n","\n","For its primary objective, uplift modeling identifies the **\"persuadables\"**, since a focus on them can lead to a more favorable outcome. Conversely, efforts directed toward the **\"lost causes\"** and **\"sure things\"** groups would become ineffective, and the **\"sleeping dogs\"** group could potentially result in negative outcomes. Uplift modeling finds practical applications in various fields\n","\n","- marketing\n","- healthcare\n","- social sciences\n","- etc.\n","\n","where it optimizes decision-making and it enhances the effectiveness of interventions.\n","\n","### Uplift modeling\n","\n","The main techniques used for uplift modeling include:\n","\n","- Meta Learner, to predict the difference between an individual's behavior when the individual undergoes a treatment, and when not undergoing a treatment.\n","\n","- Uplift Tree, a tree-based algorithm that combines both treatment/control group assignment information and response information directly into decisions about splitting criterion for a node.\n","\n","- Neural Network-based Model, which usually works with observational data. It uses deep learning to help determine the distribution of a latent variable. The latent variable represents the co-founder in the uplift modeling.\n","\n","### Uplift modeling offers benefits\n","\n","- For marketing applications, uplift modeling can help identify persuadables who can potentially be swayed to try the treatment. The outreach to identify persuadables could involve a coupon or an online advertisement, for example.\n","- For medical treatments, uplift modeling can help measure how a particular treatment can impact distinct groups. This allows for optimized target selection, to maximize the impact.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"58090d36-f361-4a8e-8a93-80fba833a43e"},{"cell_type":"markdown","source":["## Step 1: Load the data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"10b9dec8-622d-4f6c-8b4c-23dc3505ede9"},{"cell_type":"markdown","source":["#### Dataset\n","\n","The Criteo AI Lab created the dataset. That dataset has 13M rows. Each row represents one user. Each row has twelve features, a treatment indicator, and two binary labels that include visit and conversion.\n","\n","|f0|f1|f2|f3|f4|f5|f6|f7|f8|f9|f10|f11|treatment|conversion|visit|\n","|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n","\n","- **f0 - f11**: feature values (dense, floating values)\n","- **treatment**: whether or not a user was randomly targeted for treatment (e.g., advertising) (1 = treatment, 0 = control)\n","- **conversion**: whether a conversion occurred (e.g., made a purchase) for a user (binary, label)\n","- **visit**: whether a visit occurred (e.g., visit the online store) for a user (binary, label)\n","\n","#### Citation\n","\n","- Dataset homepage: https://ailab.criteo.com/criteo-uplift-prediction-dataset/\n","\n","The dataset used for this notebook requires this BibTex citation:\n","\n","    @inproceedings{Diemert2018,\n","    author = {{Diemert Eustache, Betlei Artem} and Renaudin, Christophe and Massih-Reza, Amini},\n","    title={A Large Scale Benchmark for Uplift Modeling},\n","    publisher = {ACM},\n","    booktitle = {Proceedings of the AdKDD and TargetAd Workshop, KDD, London,United Kingdom, August, 20, 2018},\n","    year = {2018}\n","    }"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"84827e55-bd4e-4ec4-b29f-54d866ac71af"},{"cell_type":"markdown","source":["> [!TIP]\n","> By defining the following parameters, you can apply this notebook on different datasets easily."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"961661cc-00ad-461a-a882-13047b7bfe92"},{"cell_type":"code","source":["IS_CUSTOM_DATA = False  # if True, dataset has to be uploaded manually by user\n","DATA_FOLDER = \"Files/uplift-modelling\"\n","DATA_FILE = \"criteo-research-uplift-v2.1.csv\"\n","\n","FEATURE_COLUMNS = [f\"f{i}\" for i in range(12)]\n","TREATMENT_COLUMN = \"treatment\"\n","LABEL_COLUMN = \"visit\"\n","\n","EXPERIMENT_NAME = \"aisample-upliftmodelling\"  # MLflow experiment name"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"daaeb770-009e-4924-85d2-57c7b6a874d5"},{"cell_type":"markdown","source":["#### Import libraries\n","\n","Before processing, you must import required Spark and SynapseML libraries. You must also import a data visualization library - for example, Seaborn, a Python data visualization library. A data visualization library provides a high-level interface to build visual resources on DataFrames and arrays. Learn more about [Spark](https://spark.apache.org/), [SynapseML](https://aka.ms/AboutSynapseML) and [Seaborn](https://seaborn.pydata.org/)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cb5b5a1c-ff44-4706-9c7d-e51c161930aa"},{"cell_type":"code","source":["import os\n","import gzip\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.window import Window\n","from pyspark.sql.types import *\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.style as style\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","from synapse.ml.featurize import Featurize\n","from synapse.ml.core.spark import FluentAPI\n","from synapse.ml.lightgbm import *\n","from synapse.ml.train import ComputeModelStatistics\n","\n","import mlflow"],"outputs":[],"execution_count":null,"metadata":{},"id":"723d006f-fc57-4207-a5e5-b604bff6df98"},{"cell_type":"markdown","source":["#### Download a dataset and upload to lakehouse\n","\n","This code downloads a publicly available version of the dataset, and then stores that data resource in a Fabric lakehouse.\n","\n","> [!IMPORTANT]\n","> **Make sure you [add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. Failure to do so will result in an error.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"81d10e4c-eb33-41e1-a3ef-4ee9c4e481b2"},{"cell_type":"code","source":["if not IS_CUSTOM_DATA:\n","    # Download demo data files into lakehouse if not exist\n","    import os, requests\n","\n","    remote_url = \"http://go.criteo.net/criteo-research-uplift-v2.1.csv.gz\"\n","    download_file = \"criteo-research-uplift-v2.1.csv.gz\"\n","    download_path = f\"/lakehouse/default/{DATA_FOLDER}/raw\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\"Default lakehouse not found, please add a lakehouse and restart the session.\")\n","    os.makedirs(download_path, exist_ok=True)\n","    if not os.path.exists(f\"{download_path}/{DATA_FILE}\"):\n","        r = requests.get(f\"{remote_url}\", timeout=30)\n","        with open(f\"{download_path}/{download_file}\", \"wb\") as f:\n","            f.write(r.content)\n","        with gzip.open(f\"{download_path}/{download_file}\", \"rb\") as fin:\n","            with open(f\"{download_path}/{DATA_FILE}\", \"wb\") as fout:\n","                fout.write(fin.read())\n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2bd24839-523a-4424-be50-ace09179278e"},{"cell_type":"markdown","source":["Start recording the runtime of this notebook."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3bb4bcc6-6c72-4370-9c00-2257432a2861"},{"cell_type":"code","source":["# Record the notebook running time\n","import time\n","\n","ts = time.time()"],"outputs":[],"execution_count":null,"metadata":{},"id":"b28508a8-71c5-4351-a0ad-9df05fcf8b8a"},{"cell_type":"markdown","source":["#### Set up the MLflow experiment tracking\n","\n","To extend the MLflow logging capabilities, autologging automatically captures the values of input parameters and output metrics of a machine learning model during its training. This information is then logged to the workspace, where the MLflow APIs or the corresponding experiment in the workspace can access and visualize it. Visit [this resource](https://aka.ms/fabric-autologging) for more information about autologging."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b9af9f97-a377-4508-af1a-6aad555e8f21"},{"cell_type":"code","source":["# Set up the MLflow experiment\n","import mlflow\n","\n","mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(disable=True)  # Disable MLflow autologging"],"outputs":[],"execution_count":null,"metadata":{},"id":"ab02f4a4-fa06-4e06-992f-82c3e3c731fc"},{"cell_type":"markdown","source":["> [!NOTE]\n","> If you want to disable Microsoft Fabric autologging in a notebook session, call `mlflow.autolog()` and set `disable=True`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3819a24e-6a70-40b0-999b-4b07b93c83d2"},{"cell_type":"markdown","source":["#### Read data from the lakehouse\n","\n","Read raw data from the lakehouse **Files** section and add additional columns for different date parts. The same information will be used to create a partitioned delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dfda728c-6966-44e6-9af5-2ed83447aac7"},{"cell_type":"code","source":["raw_df = spark.read.csv(f\"{DATA_FOLDER}/raw/{DATA_FILE}\", header=True, inferSchema=True).cache()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7b4bb2f6-71db-457f-b702-9b9bf8e303c0"},{"cell_type":"markdown","source":["## Step 2: Exploratory data analysis"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ce193d37-2b0d-4839-b9c0-66ddc2a47f89"},{"cell_type":"markdown","source":["Use the `display` command to view high-level statistics about the dataset. You can also show the Chart views to easily visualize subsets of the dataset."],"metadata":{},"id":"6490f765"},{"cell_type":"code","source":["display(raw_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{},"id":"1b1e69b2"},{"cell_type":"markdown","source":["Examine the percentage of the users that visit, the percentage of users that convert, and the percentage of the visitors that convert."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"34c36b4c-88d9-49e9-8ffe-87867a93dd7f"},{"cell_type":"code","source":["raw_df.select(\n","    F.mean(\"visit\").alias(\"Percentage of users that visit\"),\n","    F.mean(\"conversion\").alias(\"Percentage of users that convert\"),\n","    (F.sum(\"conversion\") / F.sum(\"visit\")).alias(\"Percentage of visitors that convert\"),\n",").show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f1774905-cbef-4f25-9df8-4b7a11262cbe"},{"cell_type":"markdown","source":["This analysis indicates that **4.7%** of all users visited the online store. Only **0.29%** converted (e.g., made a purchase). This means that the conversion rate of visitors is **6.2%**."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"37ad065d-2341-4e68-9c57-50911d7fc0c3"},{"cell_type":"markdown","source":["Additionally, examine the overall average treatment effect on visits and conversions. A treatment of 1 indicates the user was offered or exposed to advertising (e.g., discount coupons, gifts, or other privileges). A treatment of 0 indicates that the user was not offered or exposed to any form of advertising."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0d5be3f7-ac9c-4927-b81b-a53f673a64a6"},{"cell_type":"code","source":["raw_df.groupby(\"treatment\").agg(\n","    F.mean(\"visit\").alias(\"Percentage of users that visit\"),\n","    F.mean(\"conversion\").alias(\"Percentage of users that convert\"),\n","    (F.sum(\"conversion\") / F.sum(\"visit\")).alias(\"Percentage of visitors that convert\"),\n",").show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18807dbf-51a4-4c11-a914-5cc48f9766d4"},{"cell_type":"markdown","source":["The analysis indicates that **4.9%** of users from the treatment group - users that received the treatment, or advertising - visited the online store. Only **3.8%** of users from the control group - users that did not receive the treatment, or were not offered or exposed to advertising - did the same. Additionally, **0.31%** of all users from the treatment group converted, or made a purchase - while only **0.19%** of users from the control group did so. As a result, the conversion rate of visitors that made a purchase, who were also members of treatment group, is **6.36%**, compared to only **5.07%** for users of the control group. This means that the treatment can potentially improve the visit rate by about **1%**, and the conversion rate of visitors by about **1.3%**. This is a significant improvement."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"769f766e-8e17-4ecc-9f19-72efb7cb44e7"},{"cell_type":"markdown","source":["## Step 3: Define the model for training"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bed56605-0882-4cf0-8731-3830a912b990"},{"cell_type":"markdown","source":["#### Prepare the training and test the datasets\n","\n","You'll fit a Featurize transformer to the `raw_df` DataFrame, to extract features from the specified input columns and output those features to a new column named `features`.\n","\n","The resulting DataFrame is stored in a new DataFrame named `df`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6db58b1e-5b13-4751-91cd-bb33efce0b3c"},{"cell_type":"code","source":["transformer = Featurize().setOutputCol(\"features\").setInputCols(FEATURE_COLUMNS).fit(raw_df)\n","df = transformer.transform(raw_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"88aa91c1-149c-47f3-9385-1eb3e0fc3db4"},{"cell_type":"code","source":["# Split the DataFrame into training and test sets, with a 80/20 ratio and a seed of 42\n","train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n","\n","# Print the training and test dataset sizes\n","print(\"Size of train dataset: %d\" % train_df.count())\n","print(\"Size of test dataset: %d\" % test_df.count())\n","\n","# Group the training dataset by the treatment column, and count the number of occurrences of each value\n","train_df.groupby(TREATMENT_COLUMN).count().show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5fcc0e16-0f6b-49ba-9d0b-0f7647d163d3"},{"cell_type":"markdown","source":["#### Prepare the treatment and control datasets\n","\n","After you create the training and test datasets, you must also form the treatment and control datasets, to train the machine learning models to measure the uplift."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"af7aa060-edbd-4bca-9d89-14e25df92d42"},{"cell_type":"code","source":["# Extract the treatment and control DataFrames\n","treatment_train_df = train_df.where(f\"{TREATMENT_COLUMN} > 0\")\n","control_train_df = train_df.where(f\"{TREATMENT_COLUMN} = 0\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3dd54f60-4e77-479e-ac91-89e68fbfaa4a"},{"cell_type":"markdown","source":["Now that you prepared your data, you can proceed to train a model with LightGBM."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"97a88ea8-1981-403c-b0e0-c8a67392d1c8"},{"cell_type":"markdown","source":["#### Uplift modeling: T-Learner with LightGBM"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"30553ba4-005b-4c4f-8723-a4b8b7822645"},{"cell_type":"markdown","source":["Meta-learners are a set of algorithms, built on top of machine learning algorithms like LightGBM, Xgboost, etc. They help estimate conditional average treatment effect, or **CATE**.\n","T-learner is a meta-learner that doesn't use a single model. Instead, T-learner uses one model per treatment variable. Therefore, two models are developed and hence it is referred to as T-learner. T-learner uses multiple machine learning models to overcome the problem of entirely discarding the treatment, by forcing the learner to first split on it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3328c276-86f9-4b3b-802c-33b19d76b78e"},{"cell_type":"code","source":["mlflow.autolog(exclusive=False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3b7e2bef-b1f2-4a98-9b02-394f09c89f2c"},{"cell_type":"code","source":["classifier = (\n","    LightGBMClassifier()\n","    .setFeaturesCol(\"features\")  # Set the column name for features\n","    .setNumLeaves(10)  # Set the number of leaves in each decision tree\n","    .setNumIterations(100)  # Set the number of boosting iterations\n","    .setObjective(\"binary\")  # Set the objective function for binary classification\n","    .setLabelCol(LABEL_COLUMN)  # Set the column name for the label\n",")\n","\n","# Start a new MLflow run with the name \"uplift\"\n","active_run = mlflow.start_run(run_name=\"uplift\")\n","\n","# Start a new nested MLflow run with the name \"treatment\"\n","with mlflow.start_run(run_name=\"treatment\", nested=True) as treatment_run:\n","    treatment_run_id = treatment_run.info.run_id  # Get the ID of the treatment run\n","    treatment_model = classifier.fit(treatment_train_df)  # Fit the classifier on the treatment training data\n","\n","# Start a new nested MLflow run with the name \"control\"\n","with mlflow.start_run(run_name=\"control\", nested=True) as control_run:\n","    control_run_id = control_run.info.run_id  # Get the ID of the control run\n","    control_model = classifier.fit(control_train_df)  # Fit the classifier on the control training data"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f1586aff-d7de-4569-b250-949d40076f29"},{"cell_type":"markdown","source":["#### Use the test dataset for a prediction\n","\n","You'll use the `treatment_model` and `control_model`, both defined earlier, to transform the `test_df` test dataset. Then, you'll calculate the predicted uplift. You'll define the predicted uplift as the difference between the predicted treatment outcome and the predicted control outcome. The greater this predicted uplift difference, the greater the effectiveness of the treatment (e.g., advertising) on an individual or a sub-group.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"41aeeb3b-05ed-47c8-bd27-16717c8fc9b4"},{"cell_type":"code","source":["getPred = F.udf(lambda v: float(v[1]), FloatType())\n","\n","# Cache the resulting DataFrame for easier access\n","test_pred_df = (\n","    test_df.mlTransform(treatment_model)\n","    .withColumn(\"treatment_pred\", getPred(\"probability\"))\n","    .drop(\"rawPrediction\", \"probability\", \"prediction\")\n","    .mlTransform(control_model)\n","    .withColumn(\"control_pred\", getPred(\"probability\"))\n","    .drop(\"rawPrediction\", \"probability\", \"prediction\")\n","    .withColumn(\"pred_uplift\", F.col(\"treatment_pred\") - F.col(\"control_pred\"))\n","    .select(TREATMENT_COLUMN, LABEL_COLUMN, \"treatment_pred\", \"control_pred\", \"pred_uplift\")\n","    .cache()\n",")\n","\n","# Display the first twenty rows of the resulting DataFrame\n","display(test_pred_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"37ee9071-8119-4ca0-b213-78f6b47e12a9"},{"cell_type":"markdown","source":["#### Perform model evaluation\n","\n","Since actual uplift cannot be observed for each individual, you need to measure the uplift over a group of individuals. You'll use an Uplift Curve that plots the real, cumulative uplift across the population.\n","\n","![criteo_uplift_curve.jpeg](https://mmlspark.blob.core.windows.net/graphics/notebooks/criteo_uplift_curve.jpeg)\n","\n","The x-axis represents the ratio of the population selected for the treatment. A value of 0 suggests no treatment group - no one is exposed to, or offered, the treatment. A value of 1 suggests a full treatment group - everyone is exposed to, or offered, the treatment. The y-axis shows the uplift measure. The aim is to find the size of the treatment group, or the percentage of the population that would be offered or exposed to the treatment (e.g., advertising). This optimizes the target selection, to maximize its impact.\n","\n","First, rank the test DataFrame order by the predicted uplift. The predicted uplift is the difference between the predicted treatment outcome and the predicted control outcome."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6f783613-8e24-4cda-b6d9-3a06aad7b650"},{"cell_type":"code","source":["# Compute the percentage rank of the predicted uplift values in descending order, and display the top twenty rows\n","test_ranked_df = test_pred_df.withColumn(\"percent_rank\", F.percent_rank().over(Window.orderBy(F.desc(\"pred_uplift\"))))\n","\n","display(test_ranked_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"78061a75-5f32-422b-9555-314d4a638306"},{"cell_type":"markdown","source":["Next, calculate the cumulative percentage of visits in both the treatment and control groups."],"metadata":{},"id":"1ba35815-5909-4feb-bb9f-25fc03c44a62"},{"cell_type":"code","source":["# Calculate the number of control and treatment samples\n","C = test_ranked_df.where(f\"{TREATMENT_COLUMN} == 0\").count()\n","T = test_ranked_df.where(f\"{TREATMENT_COLUMN} != 0\").count()\n","\n","# Add columns to the DataFrame to calculate the control and treatment cumulative sum\n","test_ranked_df = (\n","    test_ranked_df.withColumn(\n","        \"control_label\",\n","        F.when(F.col(TREATMENT_COLUMN) == 0, F.col(LABEL_COLUMN)).otherwise(0),\n","    )\n","    .withColumn(\n","        \"treatment_label\",\n","        F.when(F.col(TREATMENT_COLUMN) != 0, F.col(LABEL_COLUMN)).otherwise(0),\n","    )\n","    .withColumn(\n","        \"control_cumsum\",\n","        F.sum(\"control_label\").over(Window.orderBy(\"percent_rank\")) / C,\n","    )\n","    .withColumn(\n","        \"treatment_cumsum\",\n","        F.sum(\"treatment_label\").over(Window.orderBy(\"percent_rank\")) / T,\n","    )\n",")\n","\n","# Display the first 20 rows of the dataframe\n","display(test_ranked_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{},"id":"40f3209a-647b-4047-a204-22971b41d7f5"},{"cell_type":"markdown","source":["Finally, at each percentage, calculate the uplift of the group as the difference between the cumulative percentage of visits between the treatment and control groups."],"metadata":{},"id":"e3564e70-25e8-4aa1-baf6-19bb0fa683e1"},{"cell_type":"code","source":["test_ranked_df = test_ranked_df.withColumn(\"group_uplift\", F.col(\"treatment_cumsum\") - F.col(\"control_cumsum\")).cache()\n","display(test_ranked_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{},"id":"b4fa91d5-da05-4269-b42f-05fd8dfbe9b3"},{"cell_type":"markdown","source":["Now, plot the uplift curve for the test dataset prediction. Note that you must convert the PySpark DataFrame to a Pandas DataFrame before plotting."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0e9bac10-1ae6-4051-b1ad-97ab170190d3"},{"cell_type":"code","source":["def uplift_plot(uplift_df):\n","    \"\"\"\n","    Plot the uplift curve\n","    \"\"\"\n","    gain_x = uplift_df.percent_rank\n","    gain_y = uplift_df.group_uplift\n","    # Plot the data\n","    fig = plt.figure(figsize=(10, 6))\n","    mpl.rcParams[\"font.size\"] = 8\n","\n","    ax = plt.plot(gain_x, gain_y, color=\"#2077B4\", label=\"Normalized Uplift Model\")\n","\n","    plt.plot(\n","        [0, gain_x.max()],\n","        [0, gain_y.max()],\n","        \"--\",\n","        color=\"tab:orange\",\n","        label=\"Random Treatment\",\n","    )\n","    plt.legend()\n","    plt.xlabel(\"Porportion Targeted\")\n","    plt.ylabel(\"Uplift\")\n","    plt.grid()\n","\n","    return fig, ax\n","\n","\n","test_ranked_pd_df = test_ranked_df.select([\"pred_uplift\", \"percent_rank\", \"group_uplift\"]).toPandas()\n","fig, ax = uplift_plot(test_ranked_pd_df)\n","\n","mlflow.log_figure(fig, \"UpliftCurve.png\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"09c53ccf-b86b-421b-aa67-63f15f767202"},{"cell_type":"markdown","source":["This analysis and the uplift curve both show that the top 20% population, as ranked by the prediction, would have a large gain if they received the treatment. This means that the top 20% of the population represents the **persuadables** group. Therefore, you can then set the cutoff score for the desired size of treatment group at 20%, to identify the target selection customers for the greatest impact."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a37345c8-bc29-4ccd-a6d6-5d71d26cdc70"},{"cell_type":"code","source":["cutoff_percentage = 0.2\n","cutoff_score = test_ranked_pd_df.iloc[int(len(test_ranked_pd_df) * cutoff_percentage)][\n","    \"pred_uplift\"\n","]\n","\n","print(\"Uplift scores that exceed {:.4f} map to Persuadables.\".format(cutoff_score))\n","mlflow.log_metrics(\n","    {\"cutoff_score\": cutoff_score, \"cutoff_percentage\": cutoff_percentage}\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"044c8efc-6d02-42c1-914e-407118c34939"},{"cell_type":"markdown","source":["## Step 4: Register the final ML Model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"adcc9678-b7fe-4cd8-b559-76b5606eb1a1"},{"cell_type":"markdown","source":["You'll use MLflow to track and log all experiments for both treatment and control groups. This tracking and logging includes the corresponding parameters, metrics, and the models. Note that this information is logged under the experiment name, in the workspace, for later use."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a363089c-5691-426d-a9dd-e30b87c0abc5"},{"cell_type":"code","source":["# Register the model\n","treatment_model_uri = \"runs:/{}/model\".format(treatment_run_id)\n","mlflow.register_model(treatment_model_uri, f\"{EXPERIMENT_NAME}-treatmentmodel\")\n","\n","control_model_uri = \"runs:/{}/model\".format(control_run_id)\n","mlflow.register_model(control_model_uri, f\"{EXPERIMENT_NAME}-controlmodel\")\n","\n","mlflow.end_run()"],"outputs":[],"execution_count":null,"metadata":{},"id":"71a0f059-0f53-42e7-a80f-5db480644fb1"},{"cell_type":"markdown","source":["To view your experiments:\n","1. On the left panel, select your workspace.\n","1. Find and select the experiment name, in this case _aisample-upliftmodelling_."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e1f73622-39fb-417c-a1f6-e243aec57ae4"},{"cell_type":"markdown","source":["\n","<img style=\"float: left;\" src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Uplift-experiment.png\"  width=\"75%\" height=\"30%\">\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fd88fb36-8929-4976-8c3f-93d61d8a47e7"},{"cell_type":"markdown","source":["## Step 5: Save the prediction results"],"metadata":{},"id":"7a987e7f-db47-4ef6-9bec-60b50c448eba"},{"cell_type":"markdown","source":["Microsoft Fabric offers PREDICT - a scalable function that supports batch scoring in any compute engine. It enables customers to operationalize machine learning models. Users can create batch predictions straight from a notebook or the item page for a specific model. Visit [this](https://aka.ms/fabric-predict) resource to learn more about PREDICT, and to learn how to use PREDICT in Microsoft Fabric."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"29d3cc36-e280-45ba-9166-4cc9384e4092"},{"cell_type":"code","source":["# Load the model back\n","loaded_treatmentmodel = mlflow.spark.load_model(treatment_model_uri, dfs_tmpdir=\"Files/spark\")\n","loaded_controlmodel = mlflow.spark.load_model(control_model_uri, dfs_tmpdir=\"Files/spark\")\n","\n","# Make predictions\n","batch_predictions_treatment = loaded_treatmentmodel.transform(test_df)\n","batch_predictions_control = loaded_controlmodel.transform(test_df)\n","batch_predictions_treatment.show(5)\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"7ae39e12-f39e-4cfb-bfab-4b25e3f7b53f"},{"cell_type":"code","source":["# Save the predictions in the lakehouse\n","batch_predictions_treatment.write.format(\"delta\").mode(\"overwrite\").save(\n","    f\"{DATA_FOLDER}/predictions/batch_predictions_treatment\"\n",")\n","batch_predictions_control.write.format(\"delta\").mode(\"overwrite\").save(\n","    f\"{DATA_FOLDER}/predictions/batch_predictions_control\"\n",")\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"92cfe364-aa31-44e8-ad74-28036cc83e0b"},{"cell_type":"code","source":["# Determine the entire runtime\n","print(f\"Full run cost {int(time.time() - ts)} seconds.\")"],"outputs":[],"execution_count":null,"metadata":{},"id":"7e31c67e-c44a-407a-b365-bf07cc6363ee"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{},"language":"python"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}}},"nbformat":4,"nbformat_minor":5}