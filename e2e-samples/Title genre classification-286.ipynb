{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score a text classification model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"290c610a-d11a-4c59-bc50-ebe07b6be7bc"},{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook demonstrates Microsoft Fabric end-to-end data science workflow for a text classification model. The scenario is to use word2vec and logistic regression on Spark to determine a book's genre from the British Library book dataset solely based on the book's title.\n","\n","The main steps in this notebook are:\n","\n","1. Install custom libraries\n","2. Load the data\n","3. Understand and process the data using exploratory data analysis\n","4. Train a machine learning model using `word2vec` and logistic regression and track experiments using MLflow and Fabric Autologging feature\n","5. Load the machine learning model for scoring and make predictions\n","\n","## Prerequisites\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fcef74af-ab8a-48b8-8d91-e7938ebda013"},{"cell_type":"markdown","source":["## Step 1: Install custom libraries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1adef613-bf09-4efd-8dd2-6dca7f8ca9b8"},{"cell_type":"markdown","source":["When developing a machine learning model or doing ad-hoc data analysis, you may need to quickly install a custom library (e.g., `wordcloud` in this notebook) for the Apache Spark session. To do this, there are two choices.\n","\n","1. You can use the in-line installation capabilities (e.g., `%pip`, `%conda`, etc.) to quickly get started with new libraries. Note that this installation option would install the custom libraries only in the current notebook and not in the workspace.\n","```python\n","# Use pip to install libraries\n","%pip install <library name>\n","\n","# Use conda to install libraries\n","%conda install <library name>\n"," \n","```\n","2. Alternatively, you can follow the instructions [here](https://aka.ms/fabric/create-environment) to learn how to create an environment which allows you to install libraries from public sources or upload custom libraries built by you or your organization."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0ecf049b-ad32-49fb-bac5-238ba67aad38"},{"cell_type":"markdown","source":["For the classification model in this notebook, you'll need to use the `wordcloud` library to represent the frequency of words in a text where the size of the word represents its frequency. Here, you'll use `%pip install` to install `wordcloud`.\n","\n","Note that the PySpark kernel will be restarted after `%pip install`, thus you'll need to install the library before you run any other cells.\n","\n"],"metadata":{},"id":"ed641f2d-b12a-4449-87ea-2dfb844b22b7"},{"cell_type":"code","source":["# Install wordcloud for text visualization using pip\n","%pip install wordcloud"],"outputs":[],"execution_count":null,"metadata":{},"id":"0846876f-3a76-4857-b559-fa8fa8492b95"},{"cell_type":"markdown","source":["## Step 2: Load the data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"53f62d14-dd3b-4bea-a130-ded5b6251460"},{"cell_type":"markdown","source":["### Dataset \n","\n","The dataset is from the British Library book dataset and comprises of metadata about books that have been digitized through collaboration between the British Library and Microsoft. The dataset consists of classifications, created by humans to indicate whether a book is \"fiction\" or \"nonfiction.\" With this dataset, the goal is to train a classification model that determines a book's genre solely based on its title.\n","\n","|BL record ID|Type of resource|Name|Dates associated with name|Type of name|Role|All names|Title|Variant titles|Series title|Number within series|Country of publication|Place of publication|Publisher|Date of publication|Edition|Physical description|Dewey classification|BL shelfmark|Topics|Genre|Languages|Notes|BL record ID for physical resource|classification_id|user_id|created_at|subject_ids|annotator_date_pub|annotator_normalised_date_pub|annotator_edition_statement|annotator_genre|annotator_FAST_genre_terms|annotator_FAST_subject_terms|annotator_comments|annotator_main_language|annotator_other_languages_summaries|annotator_summaries_language|annotator_translation|annotator_original_language|annotator_publisher|annotator_place_pub|annotator_country|annotator_title|Link to digitized book|annotated|\n","|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n","|014602826|Monograph|Yearsley, Ann|1753-1806|person||More, Hannah, 1745-1833 [person]; Yearsley, Ann, 1753-1806 [person]|Poems on several occasions [With a prefatory letter by Hannah More.]||||England|London||1786|Fourth edition MANUSCRIPT note|||Digital Store 11644.d.32|||English||003996603||||||||||||||||||||||False|\n","|014602830|Monograph|A, T.||person||Oldham, John, 1653-1683 [person]; A, T. [person]|A Satyr against Vertue. (A poem: supposed to be spoken by a Town-Hector [By John Oldham. The preface signed: T. A.])||||England|London||1679||15 pages (4Â°)||Digital Store 11602.ee.10. (2.)|||English||000001143||||||||||||||||||||||False|\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"11e63bb4-0303-4af1-aa2f-43bf99b06f1a"},{"cell_type":"markdown","source":["### Download dataset and upload to lakehouse\n","\n","> [!TIP]\n","> By defining the following parameters, you can apply this notebook on different datasets easily.\n"],"metadata":{},"id":"b5a41c8e-dd5d-4e8b-93f8-ff7230a52973"},{"cell_type":"code","source":["IS_CUSTOM_DATA = False  # if True, dataset has to be uploaded manually by user\n","DATA_FOLDER = \"Files/title-genre-classification\"\n","DATA_FILE = \"blbooksgenre.csv\"\n","\n","# Data schema\n","TEXT_COL = \"Title\"\n","LABEL_COL = \"annotator_genre\"\n","LABELS = [\"Fiction\", \"Non-fiction\"]\n","\n","EXPERIMENT_NAME = \"sample-aisample-textclassification\"  # MLflow experiment name"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5dc92444-8c48-4631-8f18-a3e5c42c6f1f"},{"cell_type":"markdown","source":["The following code downloads a publicly available version of the dataset and then store it in a Fabric lakehouse.\n","\n","> [!IMPORTANT]\n","> **Make sure you [add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. Failure to do so will result in an error.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"20ff966f-7adb-49ed-8871-5560ede828c5"},{"cell_type":"code","source":["if not IS_CUSTOM_DATA:\n","    # Download demo data files into lakehouse if it does not exist\n","    import os, requests\n","\n","    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/Title_Genre_Classification\"\n","    fname = \"blbooksgenre.csv\"\n","    download_path = f\"/lakehouse/default/{DATA_FOLDER}/raw\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        # Add a lakehouse if no default lakehouse has been added to the notebook\n","        # A new notebook will not link to any lakehouse by default\n","        raise FileNotFoundError(\n","            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","        )\n","    os.makedirs(download_path, exist_ok=True)\n","    if not os.path.exists(f\"{download_path}/{fname}\"):\n","        r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n","        with open(f\"{download_path}/{fname}\", \"wb\") as f:\n","            f.write(r.content)\n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"649578f1-29d7-434c-b20f-af240bb14fa7"},{"cell_type":"markdown","source":["### Import required libraries\n","\n","Prior to any processing, you need to import required libraries including those for [Spark](https://spark.apache.org/) and [SynapseML](https://aka.ms/AboutSynapseML)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d7e8bc2b-a143-4f3d-81ce-599a454cd800"},{"cell_type":"code","source":["import numpy as np\n","from itertools import chain\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import pyspark.sql.functions as F\n","\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import *\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import (\n","    BinaryClassificationEvaluator,\n","    MulticlassClassificationEvaluator,\n",")\n","\n","from synapse.ml.stages import ClassBalancer\n","from synapse.ml.train import ComputeModelStatistics\n","\n","import mlflow"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cb9f1fcf-bf37-46c2-884d-412ac210ded0"},{"cell_type":"markdown","source":["### Define hyperparameters\n","\n","Define some hyperparameters for model training.\n","\n",">[!IMPORTANT]\n","> Only modify these hyperparameters if you have an understanding of each parameter."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"350a9c2e-935e-41ab-b3fb-d34985eccfc9"},{"cell_type":"code","source":["# Hyper-parameters \n","word2vec_size = 128  # The length of the vector for each word\n","min_word_count = 3  # The minimum number of times that a word must appear to be considered\n","max_iter = 10  # The maximum number of training iterations\n","k_folds = 3  # The number of folds for cross-validation"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"afe6c1d2-d209-40b3-beed-346b1fb026c4"},{"cell_type":"markdown","source":["Start recording the time it takes to run this notebook."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e9dd6ad8-2765-490b-b19d-95abcbbb3a56"},{"cell_type":"code","source":["# Record the notebook running time\n","import time\n","\n","ts = time.time()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d40d0383-edec-4e4b-be39-d8d82676032b"},{"cell_type":"markdown","source":["### Set up the MLflow experiment tracking\n","\n","Extending the MLflow autologging capabilities, autologging works by automatically capturing the values of input parameters and output metrics of a machine learning model as it is being trained. This information is then logged to the workspace, where it can be accessed and visualized using the MLflow APIs or the corresponding experiment in the workspace. To learn more about  autologging, see [Autologging in Microsoft Fabric](https://aka.ms/fabric-autologging)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"920c1b99-fd37-4f40-8e49-1fd9ba7d9be6"},{"cell_type":"code","source":["# Set up Mlflow for experiment tracking\n","\n","mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(disable=True)  # Disable Mlflow autologging"],"outputs":[],"execution_count":null,"metadata":{},"id":"12e482a7-72f7-48ad-bd55-9bd6e61b60f3"},{"cell_type":"markdown","source":["> [!NOTE]\n","> If you want to disable Microsoft Fabric autologging in a notebook session, call `mlflow.autolog()` and set `disable=True`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1d2884de-17c8-4458-8883-5212f14c0543"},{"cell_type":"markdown","source":["### Read raw date data from the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"184a16b6-f9fd-4761-908f-c0b3d61e064b"},{"cell_type":"code","source":["raw_df = spark.read.csv(f\"{DATA_FOLDER}/raw/{DATA_FILE}\", header=True, inferSchema=True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e328d7d-3682-4b6c-b109-6c43016a25b7"},{"cell_type":"markdown","source":["## Step 3: Exploratory Data Analysis"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cc17450e-081e-4180-ae00-08608ba80899"},{"cell_type":"markdown","source":["Explore the dataset using the `display` command to view high-level statistics of the dataset or even show the chart views."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7769766c-8261-43e6-9f73-7047aa3e29ab"},{"cell_type":"code","source":["display(raw_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"e6682d98-5d51-49d0-aed5-c256e9a9c27f\",\"activityId\":\"3fd88b16-e905-4c38-99da-16b9b46411b3\",\"applicationId\":\"application_1693933166453_0001\",\"jobGroupId\":\"30\",\"advices\":{\"info\":1}}"}},"id":"b58f7e0e"},{"cell_type":"markdown","source":["###  Data preparation\n","\n","Data Preparation includes the following steps:\n","\n","- Clean the dataset\n","- Deal with dataset imbalance\n","- Tokenize the dataset\n","- Display the wordcloud\n","- Vectorize the dataset\n","\n","\n","Start cleaning the data by removing the duplicates."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f44f6626-5223-4b65-81bd-d216b492df3f"},{"cell_type":"code","source":["df = (\n","    raw_df.select([TEXT_COL, LABEL_COL])\n","    .where(F.col(LABEL_COL).isin(LABELS))\n","    .dropDuplicates([TEXT_COL])\n","    .cache()\n",")\n","\n","display(df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"32b21007-90d1-48d0-b686-7d77533c1f09"},{"cell_type":"markdown","source":["Then apply the class balancing in order to address any bias."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"db68baad-2629-4702-8227-7ce2edf0aece"},{"cell_type":"code","source":["# Create an instance of ClassBalancer and set the input column to LABEL_COL\n","cb = ClassBalancer().setInputCol(LABEL_COL)\n","\n","# Fit the ClassBalancer instance to the input DataFrame and transform the DataFrame\n","df = cb.fit(df).transform(df)\n","\n","# Display the first 20 rows of the transformed DataFrame\n","display(df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f30f8f40-7c3a-4ae8-9d4b-0d18c34d10c5"},{"cell_type":"markdown","source":["Tokenize by splitting the paragraphs and sentences into smaller units that can be more easily assigned meaning. Then remove the stopwords in order to improve the performance. Stopword removal is one of the most commonly used preprocessing steps in Natural Language Processing (NLP) applications, where the idea is to remove the words that occur commonly across all the documents in the corpus. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7f85158a-af45-476d-9904-f643f4fee54c"},{"cell_type":"code","source":["# Text transformer\n","tokenizer = Tokenizer(inputCol=TEXT_COL, outputCol=\"tokens\")\n","stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n","\n","# Build the pipeline\n","pipeline = Pipeline(stages=[tokenizer, stopwords_remover])\n","\n","token_df = pipeline.fit(df).transform(df)\n","\n","display(token_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"68a158b1-d6d5-4681-9171-38b5e9734344"},{"cell_type":"markdown","source":["Display the wordcloud for each class.\n","\n","A wordcloud is a visually prominent presentation of âkeywordsâ that appear frequently in text data. The wordcloud is effective because the rendering of keywords forms a cloud-like color picture to better capture the main text data at a glance. Learn [more about `wordcloud`](https://github.com/amueller/word_cloud).\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"09305d86-53d0-4f3a-9302-d39fdd32b63b"},{"cell_type":"code","source":["# WordCloud\n","for label in LABELS:\n","    tokens = (\n","        token_df.where(F.col(LABEL_COL) == label)\n","        .select(F.explode(\"filtered_tokens\").alias(\"token\"))\n","        .where(F.col(\"token\").rlike(r\"^\\w+$\"))\n","    )\n","\n","    top50_tokens = (\n","        tokens.groupBy(\"token\").count().orderBy(F.desc(\"count\")).limit(50).collect()\n","    )\n","\n","    # Generate a word cloud image\n","    wordcloud = WordCloud(\n","        scale=10,\n","        background_color=\"white\",\n","        random_state=42,  # Make sure the output is always the same for the same input\n","    ).generate_from_frequencies(dict(top50_tokens))\n","\n","    # Display the generated image using matplotlib\n","    plt.figure(figsize=(10, 10))\n","    plt.title(label, fontsize=20)\n","    plt.axis(\"off\")\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e2892398-bcc7-4216-9274-f28b7d76ebe9"},{"cell_type":"markdown","source":["Finally, use `word2vec` to vectorize the text. `word2vec` creates a representation of each word present in your text into a vector. Words used in similar contexts or having semantic relationships are captured effectively through their closeness in the vector space, indicating that similar words have similar word vectors."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5d515752-3aa6-4af5-98dc-b6339cb58d5d"},{"cell_type":"code","source":["# Label transformer\n","label_indexer = StringIndexer(inputCol=LABEL_COL, outputCol=\"labelIdx\")\n","vectorizer = Word2Vec(\n","    vectorSize=word2vec_size,\n","    minCount=min_word_count,\n","    inputCol=\"filtered_tokens\",\n","    outputCol=\"features\",\n",")\n","\n","# Build the pipeline\n","pipeline = Pipeline(stages=[label_indexer, vectorizer])\n","vec_df = (\n","    pipeline.fit(token_df)\n","    .transform(token_df)\n","    .select([TEXT_COL, LABEL_COL, \"features\", \"labelIdx\", \"weight\"])\n",")\n","\n","display(vec_df.limit(20))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"276d331e-be39-48f2-b5be-caa129a6d9e4"},{"cell_type":"markdown","source":["## Step 4: Model training and evaluation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fa2beda6-86e9-4eb9-8d0b-0ef7d99d4837"},{"cell_type":"markdown","source":["With your data in place, now define the model. In this section, train a logistic regression model to classify the vectorized text."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"53c24a83-5adc-4cc7-b3fa-094e6f1841fd"},{"cell_type":"markdown","source":["### Prepare training and test datasets"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c4845ef3-61e2-4d94-ab51-c846c670ddb4"},{"cell_type":"code","source":["# Split the dataset into training and test sets\n","(train_df, test_df) = vec_df.randomSplit((0.8, 0.2), seed=42)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b49c17ca-0928-42ff-ac5c-6091ef88f4a4"},{"cell_type":"markdown","source":["### Model training and machine learning experiments\n","\n","A machine learning experiment is the primary unit of organization and control for all related machine learning runs. A run corresponds to a single execution of model code. Machine learning experiment tracking refers to the process of managing all the different experiments and their components, such as parameters, metrics, models and other artifacts. Tracking enables you to organize all the required components of a specific machine learning experiment and to easily reproduce past results using saved experiments. Learn more about [machine learning experiments in Microsoft Fabric](https://aka.ms/synapse-experiment).\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"172d2c5f-2aea-441c-a1c1-aaf3c6147cb7"},{"cell_type":"code","source":["# Build the logistic regression classifier\n","lr = (\n","    LogisticRegression()\n","    .setMaxIter(max_iter)\n","    .setFeaturesCol(\"features\")\n","    .setLabelCol(\"labelIdx\")\n","    .setWeightCol(\"weight\")\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6a3fb58a-4a33-4c5f-8089-d96eedc52877"},{"cell_type":"markdown","source":["### Model training and hyperparameter tuning\n","\n","Construct a grid of parameters to search over the hyperparameters and a cross evaluator estimator to produce a CrossValidatorModel."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d51f6f84-e349-47b5-ba33-3859d95af8d0"},{"cell_type":"code","source":["# Construct a grid search to select the best values for the training parameters\n","param_grid = (\n","    ParamGridBuilder()\n","    .addGrid(lr.regParam, [0.03, 0.1])\n","    .addGrid(lr.elasticNetParam, [0.0, 0.1])\n","    .build()\n",")\n","\n","if len(LABELS) > 2:\n","    evaluator_cls = MulticlassClassificationEvaluator\n","    evaluator_metrics = [\"f1\", \"accuracy\"]\n","else:\n","    evaluator_cls = BinaryClassificationEvaluator\n","    evaluator_metrics = [\"areaUnderROC\", \"areaUnderPR\"]\n","evaluator = evaluator_cls(labelCol=\"labelIdx\", weightCol=\"weight\")\n","\n","# Construct a cross evaluator estimator\n","crossval = CrossValidator(\n","    estimator=lr,\n","    estimatorParamMaps=param_grid,\n","    evaluator=evaluator,\n","    numFolds=k_folds,\n","    collectSubModels=True,\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"75f23295-2fb6-497e-bf71-f937818ca16c"},{"cell_type":"markdown","source":["### Model evaluation\n","\n","You now have different models to compare by evaluating them on the test dataset. If a model has been well trained, it should demonstrate high performance on the relevant metrics on the validation and test datasets."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"18354ada-b462-4580-9e94-5cc073f243d6"},{"cell_type":"code","source":["def evaluate(model, df):\n","    log_metric = {}\n","    prediction = model.transform(df)\n","    for metric in evaluator_metrics:\n","        value = evaluator.evaluate(prediction, {evaluator.metricName: metric})\n","        log_metric[metric] = value\n","        print(f\"{metric}: {value:.4f}\")\n","    return prediction, log_metric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0595bc50-4279-47ac-b6c7-5881208a2d71"},{"cell_type":"markdown","source":["### Experiment tracking with MLflow\n","\n","Start the training and evaluation and use MLflow to track all experiments and log parameters, metrics, and the models. All this information is logged under the experiment name in the workspace."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0ba4838d-af96-49a8-b72a-f0bdb9c553e1"},{"cell_type":"code","source":["with mlflow.start_run(run_name=\"lr\"):\n","    models = crossval.fit(train_df)\n","    best_metrics = {k: 0 for k in evaluator_metrics}\n","    best_index = 0\n","    for idx, model in enumerate(models.subModels[0]):\n","        with mlflow.start_run(nested=True, run_name=f\"lr_{idx}\") as run:\n","            print(\"\\nEvaluating on testing data:\")\n","            print(f\"subModel No. {idx + 1}\")\n","            prediction, log_metric = evaluate(model, test_df)\n","\n","            if log_metric[evaluator_metrics[0]] > best_metrics[evaluator_metrics[0]]:\n","                best_metrics = log_metric\n","                best_index = idx\n","\n","            print(\"log model\")\n","            mlflow.spark.log_model(\n","                model,\n","                f\"{EXPERIMENT_NAME}-lrmodel\",\n","                registered_model_name=f\"{EXPERIMENT_NAME}-lrmodel\",\n","                dfs_tmpdir=\"Files/spark\",\n","            )\n","\n","            print(\"log metrics\")\n","            mlflow.log_metrics(log_metric)\n","\n","            print(\"log parameters\")\n","            mlflow.log_params(\n","                {\n","                    \"word2vec_size\": word2vec_size,\n","                    \"min_word_count\": min_word_count,\n","                    \"max_iter\": max_iter,\n","                    \"k_folds\": k_folds,\n","                    \"DATA_FILE\": DATA_FILE,\n","                }\n","            )\n","\n","    # Log the best model and its relevant metrics and parameters to the parent run\n","    mlflow.spark.log_model(\n","        models.subModels[0][best_index],\n","        f\"{EXPERIMENT_NAME}-lrmodel\",\n","        registered_model_name=f\"{EXPERIMENT_NAME}-lrmodel\",\n","        dfs_tmpdir=\"Files/spark\",\n","    )\n","    mlflow.log_metrics(best_metrics)\n","    mlflow.log_params(\n","        {\n","            \"word2vec_size\": word2vec_size,\n","            \"min_word_count\": min_word_count,\n","            \"max_iter\": max_iter,\n","            \"k_folds\": k_folds,\n","            \"DATA_FILE\": DATA_FILE,\n","        }\n","    )\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"af337e32-21a8-4764-8510-802c47b0c851"},{"cell_type":"markdown","source":["To view your experiments:\n","\n","1. On the left, select your workspace.\n","1. Find and select the experiment name, in this case _sample_aisample-textclassification_."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"59e1de64-d289-4bb3-93b4-d10abf7e5cc2"},{"cell_type":"markdown","source":["<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Title_Genre_Classification/TextClassification-experiment.png\"  width=\"70%\" height=\"30%\" title=\"Screenshot of an experiment.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b844ae7f-75e6-4400-b3f0-d7c98d269d47"},{"cell_type":"markdown","source":["## Step 5: Score and save prediction results"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e750447c-d236-48e8-ba33-de996ff5ca03"},{"cell_type":"markdown","source":["Microsoft Fabric offers a scalable function called PREDICT that supports batch scoring in any compute engine and enables customers to operationalize machine learning models. You can create batch predictions straight from a notebook or the item page for a particular model. Learn more about [PREDICT](https://aka.ms/fabric-predict) and how to use it in Microsoft Fabric.\n","\n","From the above evaluation results, model 1 has the largest Area Under the Precision-Recall Curve (AUPRC) and Area Under the Curve Receiver Operating Characteristic (AUC-ROC) metrics. Thus you should use model 1 for prediction.\n","\n","The AUC-ROC measure is widely used to assess the performance of binary classifiers. However, sometimes, it's more appropriate to evaluate the classifier based on measuring AUPRC. AUC-ROC is a chart that visualizes the trade-off between true positive rate (TPR) and false positive rate (FPR). AUPRC is a curve that combines precision (positive predictive value or PPV) and Recall (true positive rate or TPR) in a single visualization.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a53973b2-0cfb-4c18-956c-142b54d4daf6"},{"cell_type":"code","source":["# Load the best model\n","model_uri = f\"models:/{EXPERIMENT_NAME}-lrmodel/1\"\n","loaded_model = mlflow.spark.load_model(model_uri, dfs_tmpdir=\"Files/spark\")\n","\n","# Verify the loaded model\n","batch_predictions = loaded_model.transform(test_df)\n","batch_predictions.show(5)"],"outputs":[],"execution_count":null,"metadata":{},"id":"5e2bb3a3-6025-4bc8-ba62-b86116261389"},{"cell_type":"code","source":["# Code to save the userRecs into lakehouse\n","batch_predictions.write.format(\"delta\").mode(\"overwrite\").save(\n","    f\"{DATA_FOLDER}/predictions/batch_predictions\"\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ea6119f5-f101-46be-ad3b-7ab89ac7ae3d"},{"cell_type":"code","source":["# Determine the entire runtime\n","print(f\"Full run cost {int(time.time() - ts)} seconds.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"67b7e8c5-3998-43f1-841b-1b6166d2b7d3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{},"language":"python"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}}},"nbformat":4,"nbformat_minor":5}