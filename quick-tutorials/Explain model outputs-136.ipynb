{"cells":[{"cell_type":"markdown","source":["# A guide to Explainable AI with SHapley Additive exPlanations "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"63b7bff5-3bb1-4700-9e2a-dcd772f0b564"},{"cell_type":"markdown","source":["## Introduction\n","\n","This tutorial shows how to leverage SHapley Additive exPlanations (SHAP) to explain the output of machine learning models in Microsoft Fabric.\n","\n","SHAP is a method used for interpreting machine learning models by attributing the contribution of each feature to the model's output for a specific data point. In this tutorial, you use Kernel SHAP to explain a tabular classification model built from the Adults Census dataset and then visualize the explanation in the ExplanationDashboard from [Responsible AI Widgets](https://github.com/microsoft/responsible-ai-widgets) in Microsoft Fabric.\n","\n","This tutorial covers these topics:\n","\n","1. Install `raiwidgets` library\n","2. Load and process the data and train a binary classification model\n","3. Create a TabularSHAP explainer and extract SHAP values\n","4. Show how to visualize the explanation using the RAI ExplanationDashboard\n"],"metadata":{},"id":"6c289059-71d2-498b-9bed-ab63d9a13f0f"},{"cell_type":"markdown","source":["## Step 1: Install custom library"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"16647de2-6f78-4270-a695-19702766941f"},{"cell_type":"markdown","source":["Prior to process the data and train a model, you need to install a custom library for which you will use the in-line installation capabilities (e.g., `pip`, `conda`, etc.) to quickly get started. Please note that this process will solely install the custom libraries within your notebook environment, and not in the workspace.\n","\n","Additionally, please be aware that the PySpark kernel will automatically restart after executing the `%pip install` command. Therefore, it is crucial to install the desired library prior to running any other cells within your notebook.\n","\n","You'll use `%pip install` to install the `raiwidgets` library. You can follow instructions available at [Package management - Azure Synapse Analytics | Microsoft Docs](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries) for further information about how to install [\"raiwidgets\"](https://pypi.org/project/raiwidgets/) and [\"interpret-community\"](https://pypi.org/project/interpret-community/) packages."],"metadata":{},"id":"b0d7b3f6-69ff-49bb-9de9-f97ab1e00c5a"},{"cell_type":"code","source":["%pip install raiwidgets itsdangerous==2.0.1 interpret-community"],"outputs":[],"execution_count":null,"metadata":{},"id":"141a7fd5-d3d6-43c5-b819-a1603970eb28"},{"cell_type":"markdown","source":["You also need to import the required libraries from [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) and [SynapseML](https://microsoft.github.io/SynapseML/) and define some User Defined Functions (UDFs) that you will need later."],"metadata":{},"id":"5764fcfc-1eee-425c-8fff-e6e1eb69f9fc"},{"cell_type":"code","source":["from IPython.terminal.interactiveshell import TerminalInteractiveShell\n","from synapse.ml.explainers import *\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import pandas as pd\n","\n","vec_access = udf(lambda v, i: float(v[i]), FloatType())\n","vec2array = udf(lambda vec: vec.toArray().tolist(), ArrayType(FloatType()))"],"outputs":[],"execution_count":null,"metadata":{},"id":"23dfa2d5-a4a4-4206-9fce-f8ecc9c6a615"},{"cell_type":"markdown","source":["To disable Microsoft Fabric autologging in a notebook session, call `mlflow.autolog()` and set `disable=True`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e81b8b08-fbfd-4d82-9278-252c78fdc67f"},{"cell_type":"code","source":["# Set up MLflow for experiment tracking\n","import mlflow\n","\n","mlflow.autolog(disable=True)  # Disable MLflow autologging"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f02c2472-cc4b-44ca-adf7-6cbd9dd0ed70"},{"cell_type":"markdown","source":["## Step 2: Load the data and train the model"],"metadata":{},"id":"afbbf262-6bb7-4e55-a36b-6e9e76b2bf03"},{"cell_type":"markdown","source":["For this tutorial, you will use the [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). The dataset contains 32,561 rows and 14 columns/features.\n","\n","Download a publicly available version of the dataset from the blog storage and load the data as a spark DataFrame."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0d0a95f3-6b36-4484-af00-63a56920a759"},{"cell_type":"code","source":["df = spark.read.parquet(\n","    \"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\"\n",").cache()\n","\n","labelIndexer = StringIndexer(\n","    inputCol=\"income\", outputCol=\"label\", stringOrderType=\"alphabetAsc\"\n",").fit(df)\n","print(\"Label index assigment: \" + str(set(zip(labelIndexer.labels, [0, 1]))))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ea05f08b-a5a8-49ff-b8ec-229b73ca71dc"},{"cell_type":"markdown","source":["Next step is to pre-process the data (indexing categorical features and one-hot encoding them) and train a Logistic Regression model to predict the `income` label (1 or 0) based on the input features."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a2a14ce1-c53f-4783-96e2-3a50eeeb74b9"},{"cell_type":"code","source":["training = labelIndexer.transform(df)\n","display(training)\n","categorical_features = [\n","    \"workclass\",\n","    \"education\",\n","    \"marital-status\",\n","    \"occupation\",\n","    \"relationship\",\n","    \"race\",\n","    \"sex\",\n","    \"native-country\",\n","]\n","categorical_features_idx = [col + \"_idx\" for col in categorical_features]\n","categorical_features_enc = [col + \"_enc\" for col in categorical_features]\n","numeric_features = [\n","    \"age\",\n","    \"education-num\",\n","    \"capital-gain\",\n","    \"capital-loss\",\n","    \"hours-per-week\",\n","]\n","# Convert the categorical features into numerical indices\n","strIndexer = StringIndexer(\n","    inputCols=categorical_features, outputCols=categorical_features_idx\n",")\n","# Perform one-hot encoding\n","onehotEnc = OneHotEncoder(\n","    inputCols=categorical_features_idx, outputCols=categorical_features_enc\n",")\n","# Create a VectorAssembler to assemble all the one-hot encoded categorical features and numerical features into a single feature vector\n","vectAssem = VectorAssembler(\n","    inputCols=categorical_features_enc + numeric_features, outputCol=\"features\"\n",")\n","# Train a Logistic Regression model\n","lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"fnlwgt\")\n","pipeline = Pipeline(stages=[strIndexer, onehotEnc, vectAssem, lr])\n","model = pipeline.fit(training)"],"outputs":[],"execution_count":null,"metadata":{},"id":"a3a3bee8-1fbf-4a77-a145-19ccd2e2c3af"},{"cell_type":"markdown","source":["After the model is trained, you randomly select some observations to be explained."],"metadata":{},"id":"cd016fef-5c66-4ce1-b0de-d7d5341faa47"},{"cell_type":"code","source":["explain_instances = (\n","    model.transform(training).orderBy(rand()).limit(5).repartition(200).cache()\n",")\n","display(explain_instances)"],"outputs":[],"execution_count":null,"metadata":{},"id":"774854b7-e692-40c4-b45d-038c72d5a054"},{"cell_type":"markdown","source":["## Step 3: Create a TabularSHAP Explainer and extract SHAP Values"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eb5574aa-324b-481a-ac99-46042a76a432"},{"cell_type":"markdown","source":["You should create a TabularSHAP explainer by configuring it with the following parameters: set the input columns to include all the features that the model uses, specify the model itself, and indicate the target output column you intend to explain.\n","\n","In this particular scenario, your goal is to elucidate the `probability` output, which is represented as a vector with a length of 2. Your specific focus, however, is on class 1 probability. To simultaneously explain both class 0 and class 1 probabilities, you must define the `targetClasses` parameter as `[0, 1]`.\n","\n","To serve as background data for the Kernel SHAP explanation method, it's recommended to randomly sample 100 rows from the training dataset. This sampled data will be used to integrate out the effects of individual features when calculating the SHAP values."],"metadata":{},"id":"48cb9027-fa02-48e4-ad4a-b5ca40f5d5dd"},{"cell_type":"code","source":["# Compute SHAP values for the trained model\n","shap = TabularSHAP(\n","    inputCols=categorical_features + numeric_features,\n","    outputCol=\"shapValues\",\n","    numSamples=5000,\n","    model=model,\n","    targetCol=\"probability\",\n","    targetClasses=[1],\n","    backgroundData=broadcast(training.orderBy(rand()).limit(100).cache()),\n",")\n","\n","shap_df = shap.transform(explain_instances)"],"outputs":[],"execution_count":null,"metadata":{},"id":"220d1bd0-dd81-4d81-bcbc-81815585d9ca"},{"cell_type":"markdown","source":["Note that `inputCols` specifies the list of input features that you want to explain which in this case combines both the categorical and the numeric features. The `outputCol` specifies the name of the output column where SHAP values will be stored in the resulting DataFrame.\n","\n","`targetCol` is used to specify the name of the target column where the model's output (probability scores) is stored and `targetClasses` indicates the class's output (e.g., 1 in this case) that is being explained (meaning you are explaining predictions for class 1).\n","\n","Once you have the resulting DataFrame that contain the SHAP values, you can extract the class 1 probability of the model output, the SHAP values for the target class, the original features, and the true label. Then you convert it to a pandas DataFrame for visualization.\n","\n","For each observation, the first element in the SHAP values vector is the base value (the mean output of the background dataset), and each of the following element is the SHAP values for each feature."],"metadata":{},"id":"1527f437-6d8f-4530-9ec6-df38e9e00c64"},{"cell_type":"code","source":["# Choose following columns from the DataFrame\n","# \"shapValues\": The modified array of SHAP values\n","# \"probability\": The extracted class 1 probability\n","# \"label\": A column assumed to contain labels or target values\n","shaps = (\n","    shap_df.withColumn(\"probability\", vec_access(col(\"probability\"), lit(1)))\n","    .withColumn(\"shapValues\", vec2array(col(\"shapValues\").getItem(0)))\n","    .select(\n","        [\"shapValues\", \"probability\", \"label\"] + categorical_features + numeric_features\n","    )\n",")\n","\n","shaps_local = shaps.toPandas()\n","shaps_local.sort_values(\"probability\", ascending=False, inplace=True, ignore_index=True) # Arrange with the highest probabilities at the top\n","pd.set_option(\"display.max_colwidth\", None)\n","shaps_local"],"outputs":[],"execution_count":null,"metadata":{},"id":"de408a94-cca0-4e66-bbb2-99621e29c036"},{"cell_type":"markdown","source":["## Step 4: Visualize the explanation using the RAI ExplanationDashboard\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"da94d029-f7b7-4353-9c94-720a842006ee"},{"cell_type":"markdown","source":["You can visualize the explanation in [interpret-community format](https://github.com/interpretml/interpret-community) in the [ExplanationDashboard](https://github.com/microsoft/responsible-ai-widgets/)."],"metadata":{},"id":"714a207e-f584-47b3-8704-1c9d2be394f4"},{"cell_type":"code","source":["import numpy as np\n","\n","features = categorical_features + numeric_features\n","features_with_base = [\"Base\"] + features\n","\n","rows = shaps_local.shape[0]\n","\n","local_importance_values = shaps_local[[\"shapValues\"]] # Extract the \"shapValues\" column from the \"shaps_local\" DataFrame\n","eval_data = shaps_local[features]\n","true_y = np.array(shaps_local[[\"label\"]])"],"outputs":[],"execution_count":null,"metadata":{},"id":"4acbacb4-5892-4e4b-af58-b5d2c8e60339"},{"cell_type":"markdown","source":["Process the SHAP values stored to separate the bias values (likely representing the base prediction) and the actual importance values for each data point and class. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"75e78146-a3de-4a7a-8321-b2ebfe0f750a"},{"cell_type":"code","source":["list_local_importance_values = local_importance_values.values.tolist()\n","converted_importance_values = []\n","bias = []\n","for classarray in list_local_importance_values:\n","    for rowarray in classarray:\n","        converted_list = rowarray.tolist()\n","        # The bias values are stored in the bias list\n","        bias.append(converted_list[0])\n","        # Remove the bias from local importance values\n","        del converted_list[0]\n","        # Importance values are stored in the converted_importance_values list\n","        converted_importance_values.append(converted_list)"],"outputs":[],"execution_count":null,"metadata":{},"id":"ce8a6981-9775-4b9c-8c6a-922da989bf65"},{"cell_type":"markdown","source":["Create a global explanation that is based on feature importance values (SHAP values), evaluation data, and expected values (bias terms)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"19481076-28eb-45cd-b70c-29b8b07eea11"},{"cell_type":"code","source":["from interpret_community.adapter import ExplanationAdapter\n","\n","adapter = ExplanationAdapter(features, classification=True) # List of features used in the explanation\n","# eval_data is the dataset used to train or test the machine learning model\n","global_explanation = adapter.create_global(\n","    converted_importance_values, eval_data, expected_values=bias\n",")"],"outputs":[],"execution_count":null,"metadata":{},"id":"45dfe570-7335-40fd-b9dd-358a9e1e7174"},{"cell_type":"markdown","source":["View the global importance values.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3c3c658c-5c04-4861-be40-6a54cad1746a"},{"cell_type":"code","source":["global_explanation.global_importance_values"],"outputs":[],"execution_count":null,"metadata":{},"id":"1756806f-a32d-4ae6-afd8-bef21ec0295b"},{"cell_type":"markdown","source":["View the local importance values."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"962a820c-8e7b-43ab-86d2-f4b59c952db8"},{"cell_type":"code","source":["global_explanation.local_importance_values"],"outputs":[],"execution_count":null,"metadata":{},"id":"39c69bce-708a-49d8-b083-276a3dfc1940"},{"cell_type":"code","source":["class wrapper(object):\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def predict(self, data):\n","        sparkdata = spark.createDataFrame(data)\n","        return (\n","            model.transform(sparkdata)\n","            .select(\"prediction\")\n","            .toPandas()\n","            .values.flatten()\n","            .tolist()\n","        )\n","\n","    def predict_proba(self, data):\n","        sparkdata = spark.createDataFrame(data)\n","        prediction = (\n","            model.transform(sparkdata)\n","            .select(\"probability\")\n","            .toPandas()\n","            .values.flatten()\n","            .tolist()\n","        )\n","        proba_list = [vector.values.tolist() for vector in prediction]\n","        return proba_list"],"outputs":[],"execution_count":null,"metadata":{},"id":"3b953799-369c-41a0-af02-0319e7756951"},{"cell_type":"markdown","source":["The following shows how the final results using the kernel SHAP will look like. You can select the feature of your interest, choose the chart type, etc. to gain valuable insights about the impact of different features.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e2e320e2-82c4-4c4d-ae34-122f4ef6cc08"},{"cell_type":"code","source":["# View the explanation in the ExplanationDashboard\n","from raiwidgets import ExplanationDashboard\n","\n","ExplanationDashboard(\n","    global_explanation, wrapper(model), dataset=eval_data, true_y=true_y\n",")"],"outputs":[],"execution_count":null,"metadata":{},"id":"6639a2cc-a807-4e34-855c-6e7bfc050857"},{"cell_type":"markdown","source":["## Summary of the learnings\n","\n","In summary, in this tutorial you have learned how to leverage kernel SHAP to provide a holistic and actionable understanding of ML models by quantifying feature importance, promoting model transparency, and facilitating model improvement and debugging. \n","\n","Kernel SHAP is a technique that helps explain the predictions of complex models by attributing the contribution of each feature to the model's output. It uses a kernel-based approach to estimate feature importance, providing insights into how different input variables influence the model's decisions. This interpretability tool aids in understanding and debugging machine learning models, making them more transparent and trustworthy.\n","\n","Through the practical illustrations presented above, you've acquired the skills to effectively utilize Kernel SHAP, ensuring the reliability and alignment of machine learning models with their intended goals."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"36a28e54-af78-4a86-bd58-bcd0691357d6"}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Interpretability - Tabular SHAP explainer","notebookOrigID":4343954975413564,"widgets":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"host":{},"language":"python"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}}},"nbformat":4,"nbformat_minor":5}